{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create an environment with conda:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! cd ./text-metrics-main\n",
    "# ! conda env create -f environment.yml  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pass all these lines to start:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm\n",
    "# !python --version\n",
    "# import spacy\n",
    "# from text_metrics.utils import get_metrics\n",
    "# %pip install accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/naomi/miniconda3/envs/language_cognition_env/lib/python3.9/site-packages/huggingface_hub-0.24.0-py3.8.egg/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import text_metrics.utils as tm_utils\n",
    "\n",
    "# tiny gpt2 model:\n",
    "model_names = [\"EleutherAI/pythia-70m\", \"EleutherAI/pythia-160m\", \"EleutherAI/pythia-410m\"]\n",
    "\n",
    "models_tokenizers = [tm_utils.init_tok_n_model(model_name) for model_name in model_names]\n",
    "tokenizers = [tokenizer for tokenizer, _ in models_tokenizers]\n",
    "models = [model for _, model in models_tokenizers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Length</th>\n",
       "      <th>Wordfreq_Frequency</th>\n",
       "      <th>subtlex_Frequency</th>\n",
       "      <th>EleutherAI/pythia-70m_Surprisal</th>\n",
       "      <th>EleutherAI/pythia-160m_Surprisal</th>\n",
       "      <th>EleutherAI/pythia-410m_Surprisal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>113,</td>\n",
       "      <td>3</td>\n",
       "      <td>17.482668</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>16.033961</td>\n",
       "      <td>16.777689</td>\n",
       "      <td>15.692215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>115,</td>\n",
       "      <td>3</td>\n",
       "      <td>17.482668</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.702277</td>\n",
       "      <td>6.542616</td>\n",
       "      <td>7.405830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>117,</td>\n",
       "      <td>3</td>\n",
       "      <td>17.482668</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.780134</td>\n",
       "      <td>3.935068</td>\n",
       "      <td>3.290075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>and</td>\n",
       "      <td>3</td>\n",
       "      <td>5.282088</td>\n",
       "      <td>6.186248</td>\n",
       "      <td>4.973053</td>\n",
       "      <td>6.991700</td>\n",
       "      <td>6.682996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>118</td>\n",
       "      <td>3</td>\n",
       "      <td>17.482668</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.312808</td>\n",
       "      <td>1.396066</td>\n",
       "      <td>1.103458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>are</td>\n",
       "      <td>3</td>\n",
       "      <td>7.506353</td>\n",
       "      <td>7.548023</td>\n",
       "      <td>5.524216</td>\n",
       "      <td>4.554984</td>\n",
       "      <td>2.759386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>36.541209</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.416397</td>\n",
       "      <td>17.073027</td>\n",
       "      <td>9.195761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The</td>\n",
       "      <td>3</td>\n",
       "      <td>4.218934</td>\n",
       "      <td>5.048944</td>\n",
       "      <td>4.654328</td>\n",
       "      <td>6.680933</td>\n",
       "      <td>5.536911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>International</td>\n",
       "      <td>13</td>\n",
       "      <td>12.092365</td>\n",
       "      <td>16.065472</td>\n",
       "      <td>7.371607</td>\n",
       "      <td>7.704390</td>\n",
       "      <td>7.434474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Union</td>\n",
       "      <td>5</td>\n",
       "      <td>13.024678</td>\n",
       "      <td>15.449666</td>\n",
       "      <td>2.442083</td>\n",
       "      <td>3.613493</td>\n",
       "      <td>4.473687</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Word  Length  Wordfreq_Frequency  subtlex_Frequency  \\\n",
       "0           113,       3           17.482668           0.000000   \n",
       "1           115,       3           17.482668           0.000000   \n",
       "2           117,       3           17.482668           0.000000   \n",
       "3            and       3            5.282088           6.186248   \n",
       "4            118       3           17.482668           0.000000   \n",
       "5            are       3            7.506353           7.548023   \n",
       "6            ...       0           36.541209           0.000000   \n",
       "7            The       3            4.218934           5.048944   \n",
       "8  International      13           12.092365          16.065472   \n",
       "9          Union       5           13.024678          15.449666   \n",
       "\n",
       "   EleutherAI/pythia-70m_Surprisal  EleutherAI/pythia-160m_Surprisal  \\\n",
       "0                        16.033961                         16.777689   \n",
       "1                         9.702277                          6.542616   \n",
       "2                         3.780134                          3.935068   \n",
       "3                         4.973053                          6.991700   \n",
       "4                         2.312808                          1.396066   \n",
       "5                         5.524216                          4.554984   \n",
       "6                        11.416397                         17.073027   \n",
       "7                         4.654328                          6.680933   \n",
       "8                         7.371607                          7.704390   \n",
       "9                         2.442083                          3.613493   \n",
       "\n",
       "   EleutherAI/pythia-410m_Surprisal  \n",
       "0                         15.692215  \n",
       "1                          7.405830  \n",
       "2                          3.290075  \n",
       "3                          6.682996  \n",
       "4                          1.103458  \n",
       "5                          2.759386  \n",
       "6                          9.195761  \n",
       "7                          5.536911  \n",
       "8                          7.434474  \n",
       "9                          4.473687  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Small test:\n",
    "\n",
    "text = \"113, 115, 117, and 118 are ... The International Union\"\n",
    "\n",
    "surp_res = tm_utils.get_metrics(\n",
    "    text=text,\n",
    "    models=models,\n",
    "    tokenizers=tokenizers,\n",
    "    model_names=model_names,\n",
    "    parsing_model=spacy.load(\"en_core_web_sm\"),\n",
    "    add_parsing_features=False,\n",
    ")\n",
    "\n",
    "surp_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Corpus from CELER:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['DATA_FILE', 'list', 'dataset_version', 'trial', 'shared_text',\n",
       "       'sentenceid', 'IA_ID', 'IA_LABEL', 'IA_LEFT', 'IA_RIGHT',\n",
       "       'IA_FIRST_FIXATION_X', 'IA_DWELL_TIME', 'IA_DWELL_TIME_%',\n",
       "       'IA_FIRST_FIXATION_DURATION', 'IA_FIRST_FIXATION_INDEX',\n",
       "       'IA_FIRST_FIXATION_PREVIOUS_FIX_IA',\n",
       "       'IA_FIRST_FIXATION_PREVIOUS_IAREAS',\n",
       "       'IA_FIRST_FIXATION_VISITED_IA_COUNT', 'IA_FIRST_FIXATION_RUN_INDEX',\n",
       "       'IA_FIRST_FIX_PROGRESSIVE', 'IA_FIRST_SACCADE_ANGLE',\n",
       "       'IA_FIRST_RUN_LANDING_POSITION', 'IA_FIRST_RUN_DWELL_TIME',\n",
       "       'IA_FIRST_RUN_FIXATION_COUNT', 'IA_FIRST_RUN_LAUNCH_SITE',\n",
       "       'IA_REGRESSION_PATH_DURATION', 'IA_REGRESSION_IN',\n",
       "       'IA_REGRESSION_IN_COUNT', 'IA_REGRESSION_OUT', 'IA_REGRESSION_OUT_FULL',\n",
       "       'IA_REGRESSION_OUT_COUNT', 'IA_REGRESSION_OUT_FULL_COUNT',\n",
       "       'IA_FIXATION_COUNT', 'IA_RUN_COUNT', 'IA_SKIP', 'IP_START_TIME',\n",
       "       'IP_END_TIME', 'EYE_USED', 'TRIAL_FIXATION_COUNT', 'TRIAL_IA_COUNT',\n",
       "       'TRIAL_DWELL_TIME', 'correct_answer', 'key_pressed', 'sentence',\n",
       "       'question', 'answered_correctly', 'WORD_NORM', 'OOV_BLLIP',\n",
       "       'FREQ_BLLIP', 'OOV_SUBTLEX', 'FREQ_SUBTLEX', 'OOV_WEB', 'FREQ_WEB',\n",
       "       'SURP_KENLM', 'SURP_LSTM', 'SURP_GPT2', 'WORD_LEN'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path = \"celer/data_v2.0/sent_ia.tsv\"\n",
    "\n",
    "full_df = pd.read_csv(path, sep=\"\\t\")\n",
    "full_df.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28208 648696\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "### get surprisal values for text:\n",
    "sentences = full_df['sentence'].unique().tolist()\n",
    "print(len(sentences), len(full_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {}\n",
    "for i, sentence in enumerate(sentences):\n",
    "    try:\n",
    "        surp_res = tm_utils.get_metrics(\n",
    "            text=sentence,\n",
    "            models=models,\n",
    "            tokenizers=tokenizers,\n",
    "            model_names=model_names,\n",
    "            parsing_model=spacy.load(\"en_core_web_sm\"),\n",
    "            add_parsing_features=False,\n",
    "        )\n",
    "        result[sentence] = surp_res\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error at {sentence}\")\n",
    "        print(e)\n",
    "        print(i)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# save results:\n",
    "with open(\"surprisal_results.pkl\", \"wb\") as f:\n",
    "    pickle.dump(result, f)\n",
    "\n",
    "# load results:\n",
    "result = pickle.load(open(\"surprisal_results.pkl\", \"rb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "317531it [00:17, 17872.67it/s]\n"
     ]
    }
   ],
   "source": [
    "### write to tsv line by line (due to size constraints):\n",
    "\n",
    "# import csv\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# with open(\"surp_data/final_surp_df3.tsv\", \"w\") as f:\n",
    "#     writer = csv.writer(f, delimiter='\\t')\n",
    "    \n",
    "#     keys = df_smaller.keys().tolist() + [\"Wordfreq_Frequency\", \"subtlex_Frequency\", \"EleutherAI/pythia-70m_Surprisal\", \"EleutherAI/pythia-160m_Surprisal\", \"EleutherAI/pythia-410m_Surprisal\"]\n",
    "#     writer.writerow(keys)\n",
    "\n",
    "#     j = 0 \n",
    "#     for i, row in tqdm(df_smaller.iterrows()):\n",
    "#         answer_df_row = answer_df.iloc[j]\n",
    "#         if row['IA_LABEL'] == answer_df_row['Word']:\n",
    "#             answer_df_row = answer_df_row.drop(labels=['Word', 'Length'])\n",
    "#             writer.writerow(row.tolist() + answer_df_row.tolist())\n",
    "#             j += 1\n",
    "#         else:\n",
    "#             writer.writerow(row.tolist() +  [\"NaN\"]*5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
